{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Measuring the Z boson mass\n",
    " This notebook contains basic anlaysis as described in the lab manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0. Importing useful packages\n",
    "\n",
    " Most python scripts start with 'import' statements like this that make useful packages available. Examples are 'uproot' which reads in our data files and allows them to be read, 'numpy' is a powerful and popular package for fast manipulation of arrays and 'scipy.stats' is useful for statistical analysis - we'll use it to generate and fit functions to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "import uproot as uproot\n",
    "import vector\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import cauchy\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Access the data file and convert into a 'pandas dataframe'.\n",
    "\n",
    " - Here the file is converted into the very poweful 'pandas dataframe' format which is used a lot in modern\n",
    "     statistical analysis. You can think of the dataframe as a super-charged spreadsheet. Each row represents \n",
    "     a proton-proton collision event, and every column represents a piece of information ('observable') we have\n",
    "     recorded from that event.\n",
    "\n",
    " - We are going to work with 7 observables: the components of the muon's four-vector, the charge of the muon and two     extra variables related to how isolated the muon is from other particles. As we will have two muons produced in       each event, that makes 14 observables per event.\n",
    " \n",
    " \n",
    "The first thing we want to do is read our data file. Remember it contains ~500k proton collision events from the ATLAS Open Data pre-selected so that each event contains two muon candidates with transverse mometum greater than 20000 MeV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " NOTE - NONE OF THE REST OF THIS NOTEBOOK WILL RUN UNTIL YOU DOWNLOAD THE INPUT DATA AND SIMULATION FILES FROM HERE. \n",
    "\n",
    " - https://uctcloud-my.sharepoint.com/:f:/g/personal/01466689_wf_uct_ac_za/En-_dYktEINOu3XP2z3VDK8B3NCT_Wj7shIBVCcV72SkAg?e=cu1Ldy\n",
    "\n",
    " - PLACE ALL FILES IN THE SAME DIRECTORY AS THIS NOTEBOOK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsData = uproot.open(\"data_Skim_mumu.root\")[\"mini\"] #this command tells uproot where to find the file\n",
    "\n",
    "df = eventsData.arrays([\"muon_E\", \"muon_pt\", \"muon_phi\", \"muon_eta\", \"muon_charge\", \"muon_etcone20\", \"muon_ptcone30\"], library=\"pd\")\n",
    "print(\"File has been successfully opened!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said dataframes are like spreadsheets and we can see that structure directly by simply typing the name of the variable representing the dataframe (\"df\") into the prompt.\n",
    "We'll see the familiar row/column structure with our twelve observables and lots of events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see each event has values of the variables for \"muon[0]\" and \"muon[1]\". These refer to the muon\n",
    "with the larger and smaller pt values respectively.\n",
    "\n",
    "The next thing we want to do is apply some more criteria to our data so that it is dominated as much as possible by the Z->mumu process, while still retaining as many events as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Apply event selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cut0: we require the charges of the muons to be opposite (by requiring the sum to be 0)\n",
    "cuts0 = df[ df['muon_charge'].apply(lambda x: x[0] + x[1] == 0)]\n",
    "\n",
    "#cut1: we require the transervse momentum of each muon to be above 20000 MeV'\n",
    "cuts1 = cuts0[(cuts0[\"muon_pt\"]).apply(lambda x: x[0] > 20000) & (cuts0[\"muon_pt\"]).apply(lambda x: x[1] > 20000)]\n",
    "\n",
    "#cut2: we require the absolute pseudorapidity of each muon to be below 2.4'\n",
    "\n",
    "cuts2 = cuts1[(cuts1[\"muon_eta\"]).apply(lambda x: np.abs(x[0]) < 2.4) & (cuts1[\"muon_eta\"]).apply(lambda x: np.abs(x[0]) < 2.4)  ]\n",
    "\n",
    "# cuts 3&4: we require the muons to be isolated from other particles in the event these criteria will be  explored later in step 7, for now leave them commented out.\n",
    "# For these cuts, we will place criteria on the ratios between the pt and two other variables in the dataframe: 'muon_etcone20' and 'muon_ptcone30'. \n",
    "# Hence we take a slightly different approach by first calculating these ratios for each muon and storing each of them as new separate columns in the dataframe.\n",
    "# We then apply the criteria as before.\n",
    "\n",
    "# For technical reasons Pandas can run intro trouble with complicatied 'slicing' of alrady sliced dataframes, so we first make a new copt of the dataframe.\n",
    "#cuts2_copy = cuts2.copy()\n",
    "\n",
    "# we add new columns for our first ratio\n",
    "#cuts2_copy.loc[:, 'iso_et20_mu0'] = (pd.Series(cuts2['muon_etcone20']).apply(lambda x: x[0]) / pd.Series(cuts2['muon_pt']).apply(lambda x: x[0]))\n",
    "#cuts2_copy.loc[:, 'iso_et20_mu1'] = (pd.Series(cuts2['muon_etcone20']).apply(lambda x: x[1]) / pd.Series(cuts2['muon_pt']).apply(lambda x: x[1]))\n",
    "\n",
    "# we place our criteria on the newly created columns\n",
    "#cuts3 = cuts2_copy[(cuts2_copy['iso_et20_mu0'] < 0.1) & (cuts2_copy['iso_et20_mu1'] < 0.1)]\n",
    "\n",
    "# same proceedure for the other ratio\n",
    "#cuts3_copy = cuts3.copy()\n",
    "#cuts3_copy.loc[:, 'iso_pt20_mu0'] = (pd.Series(cuts3_copy['muon_ptcone30']).apply(lambda x: x[0]) / pd.Series(cuts3_copy['muon_pt']).apply(lambda x: x[0]))\n",
    "#cuts3_copy.loc[:, 'iso_pt20_mu1'] = (pd.Series(cuts3_copy['muon_ptcone30']).apply(lambda x: x[1]) / pd.Series(cuts3_copy['muon_pt']).apply(lambda x: x[1]))\n",
    "#cuts4 = cuts3_copy[(cuts3_copy['iso_et20_mu0'] < 0.1) & (cuts3_copy['iso_et20_mu1'] < 0.1)]\n",
    "\n",
    "#we copy our finally selected dataframe to a new variable 'finalData' for convenience\n",
    "finalData =  cuts2 # you will have to change this line when you want to include the muon \n",
    "#isolation criteria, i.e., \"finalData =  cuts4\" \n",
    "\n",
    "#let's check how many events we have selected after all our criteria have been applied\n",
    "print(\"Number of selected events = \" + str(len(finalData.index))) # this should be 540579! \n",
    "\n",
    "# and have a look at our finally selected dataframe\n",
    "finalData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 3. 'Reconstruct' the Z boson by adding the four-vectors of each muon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we make arrays of four-vectors for the muons. we make separate arrays for leading and sub-leading muons\n",
    "pt0 = finalData['muon_pt'].apply(lambda x: x[0]).to_numpy()\n",
    "eta0 = finalData['muon_eta'].apply(lambda x: x[0]).to_numpy()\n",
    "phi0 = finalData['muon_phi'].apply(lambda x: x[0]).to_numpy()\n",
    "E0 = finalData['muon_E'].apply(lambda x: x[0]).to_numpy()\n",
    "\n",
    "pt1 = finalData['muon_pt'].apply(lambda x: x[1]).to_numpy()\n",
    "eta1 = finalData['muon_eta'].apply(lambda x: x[1]).to_numpy()\n",
    "phi1 = finalData['muon_phi'].apply(lambda x: x[1]).to_numpy()\n",
    "E1 = finalData['muon_E'].apply(lambda x: x[1]).to_numpy()\n",
    "\n",
    "lvArray0 = vector.array(\n",
    "    {\n",
    "        \"pt\": pt0,\n",
    "        \"phi\": phi0,\n",
    "        \"eta\": eta0,\n",
    "        \"E\": E0,\n",
    "    }\n",
    ")\n",
    "\n",
    "lvArray1 = vector.array(\n",
    "    {\n",
    "        \"pt\": pt1,\n",
    "        \"phi\": phi1,\n",
    "        \"eta\": eta1,\n",
    "        \"E\": E1,\n",
    "    }\n",
    ")\n",
    "\n",
    "# the vector package conveniently allows us to simply add the arrays to get an array of four-vectors representing the dimuon system in each event\n",
    "lvArray = lvArray0 + lvArray1\n",
    "lvArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.  Make histogram plots of the kinematic information of the muon pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make some simple histograms of the kineamtic information associated with the muons.\n",
    "# as an example I look at muon[0] only, but you can look at muon[1] and compare the distributions.\n",
    "\n",
    "# plot the mu[0] pt histogram.\n",
    "plt.figure()\n",
    "plt.xlabel(\"mu pt\")\n",
    "plt.ylabel(\"events per bin\")\n",
    "plt.hist(lvArray0.pt, bins=35, range=[0,120000], alpha=0.6, color='g')\n",
    "\n",
    "# plot the mu[0] pseudorapdity histogram.\n",
    "plt.figure()\n",
    "plt.xlabel(\"mu eta\")\n",
    "plt.ylabel(\"events per bin\")\n",
    "plt.hist(lvArray0.eta, bins=25, range=[-3.0,3.0], alpha=0.6, color='g')\n",
    "\n",
    "# plot the mu[0] phi histogram.\n",
    "plt.figure()\n",
    "plt.xlabel(\"mu phi\")\n",
    "plt.ylabel(\"events per bin\")\n",
    "plt.hist(lvArray0.phi, bins=15, range=[-3.14,3.14], alpha=0.6, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Make histogram plots of the kinematic information of the muons.\n",
    "- now that we have reconstructed four-vectors of the dimuon system and know how to make histogram plots,let's make a histogram plot of the dimuon mass distribution and see if the Cauchy distribution can be seen and if the peak is somehwhere around 90 GeV\n",
    "\n",
    "- we will compare this histogram to a cauchy pdf. However, pdfs are always normalised to an area of 1 while our data represents hundreds of thousands of events, so the data histogram will surely have an integral much larger than 1. Therefore we must first calculate this integral use it to scale our pdf to allow a fair visual comparison between the data histogram and the predicted Cauchy distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we set up a few parmaters for our histogram, min and max of the x-axis (mass)\n",
    "minMass = 71000\n",
    "maxMass = 110000\n",
    "\n",
    "#the number of bins\n",
    "nBins = 250 \n",
    "\n",
    "# creating the histogram as two arrays (bin edges & counts in the bins) numpy\n",
    "countsData, edges = np.histogram(lvArray.mass, bins=nBins, range=(minMass, maxMass))\n",
    "\n",
    "# get the width of each bin\n",
    "bin_width = edges[1] - edges[0]\n",
    "# sum over number in each bin and multiply by bin width, which can be factored out.\n",
    "# This gives us the integral\n",
    "integral = bin_width * sum(countsData[0:nBins])\n",
    "\n",
    "# we can make an array of the centre of each bin directly from the edges array this will be useful in plotting our pdf\n",
    "centres = (edges[1:] + edges[:-1]) / 2\n",
    "\n",
    "#fit a Cauchy distributions to the dimuon mass data\n",
    "#mu, std = cauchy.fit(lvArray.mass)\n",
    "#print(\"mu = \" + str(mu))\n",
    "#print(\"std = \" + str(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are almost ready to compare our data histogram with the predicted Cauchy distribution but we know the Cauchy pdf has two free parameters: the mean (mu) and standard deviation (sigma). what should we choose for these values for a first comparison? Why not the world-average values published by the Particle Data group below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mZPDG  = 91187.6\n",
    "sigmaZPDG = 2495.2\n",
    "\n",
    "#let's make the comparison plot\n",
    "plt.figure()\n",
    "p = (cauchy.pdf(centres, mZPDG, sigmaZPDG) * integral)\n",
    "plt.plot(centres, p, 'k',label=\"Cauchy pdf (PDG values)\", linewidth=2)\n",
    "plt.hist(lvArray.mass, bins=nBins, range=[minMass, maxMass],label=\"ATLAS Data\", alpha=0.6, color='g')\n",
    "plt.legend()\n",
    "plt.xlabel(\"mumu mass [MeV]\")\n",
    "plt.ylabel(\"# events\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a clear peak and a shape that looks very much like a Cauchy distrubtion. So we can be confident that our data conatains the decays of a heavy particle to pairs of muons. However the data does not perfectly agree with the predicted shape of Cauchy distributions with the PDG values for the mass and width of the Z boson.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fitting the Cauchy model to the data\n",
    "In the next part of this notebook we will try to assess if we can find values of the Z mass which result in a better 'agreement' between the data and Cauchy model. If we trust the Cauchy model completely, then this proceedure is a measurement of the Z mass. We'll define the agreement between our data and model with the chi-squared function. The function takes in arrays of event counts ('obs') and predictions ('preds') corresponding to the bins of the dimuon mass distribution. The function returns the squared difference between the event count and prediction divided by the variance on the prediction, summed over all bins, otherwise known as the chi-squared function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chi-squared function\n",
    "def calcChiSq(obs, preds):\n",
    "    chiSq = 0.0\n",
    "    ndf = len(obs)\n",
    "    for bin in range(0, len(obs)):\n",
    "        diff = preds[bin] - obs[bin]\n",
    "        var = ( np.abs(preds[bin])) # pearson's chi2 (approximate the prediction's variance as poisson, i.e., take the prediction as the variance)\n",
    "        if (var != 0):\n",
    "            chiSq += (diff**2)/(var)\n",
    "    return chiSq, ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the fit we will:\n",
    "- We'll now loop over a range of $m_{Z}$ values, \n",
    "- Generate a cauchy distribution for each value and evaluate the $\\chi^{2}$ between that distribution and the data histogram.\n",
    "- find the $m_{Z}$ value that gives the smallest value of the $\\chi^{2}$\n",
    "- plot the $\\Delta \\chi^{2}$ vs the $m_{Z}$, and use the $\\emph{critical values}$ of this curve to evaluate the uncertainties on $m_{Z}$\n",
    "- compare the data histogram to the fitted Cauchy distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some arrays to hold the z mass and chi-squared values\n",
    "mZAr = np.array([])\n",
    "chi2Ar = np.array([])\n",
    "\n",
    "#deciding the range of z mass values and how fine the steps in this range will be\n",
    "minMz = 90730\n",
    "maxMz = 90765\n",
    "step = 1\n",
    "\n",
    "#vairables to use in the loop to find value of z mass that gives smallest chi2\n",
    "bestFitMz1D = 0.0\n",
    "minChi2 = 10000000\n",
    "\n",
    "for mZ in range(minMz, maxMz, step): #starting loop\n",
    "    countsPDF = (cauchy.pdf(centres, mZ, sigmaZPDG) * integral) #building pdf for these values\n",
    "    #countsPDF = (norm.pdf(centres, mZ, sigmaZPDG) * integral) #building pdf for these values\n",
    "    chi2, ndf = calcChiSq(countsData, countsPDF) # call chi2 function to get chi2 for this particular pdf and data\n",
    "    mZAr = np.append(mZAr, mZ) #adding values to arrays\n",
    "    chi2Ar = np.append(chi2Ar, chi2)\n",
    "    if(chi2 < minChi2): #keeping track of what is the smallest chi2 value we have found\n",
    "        minChi2 = chi2\n",
    "        bestFitMz1D = mZ\n",
    "                \n",
    "# as we will only be interested in the change in chi2 w.r.t. mZ,  we can rescale all the chi2\n",
    "#values such that the minimum chi2 value is 0. \n",
    "chi2Ar = chi2Ar - minChi2\n",
    "\n",
    "#we expect the chi2 vs. mZ curve to be quadratic, so let's fit that fucntion to it.\n",
    "z = np.polyfit(mZAr, chi2Ar, 2) #\"2\" for a second-order polynomial\n",
    "p = np.poly1d(z)\n",
    "\n",
    "#lets plot our chi2 values vs mz along with the fit\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mZAr, chi2Ar, 'r+', lw=5, alpha=0.6, label=\"chi2 evaluations\")\n",
    "ax.plot(mZAr, p(mZAr), 'b-', lw=5, alpha=0.6, label=\"quadratic fit\")\n",
    "\n",
    "# we can display the estimated uncertianty on mZ via critical values () of the delta chi-squared curve\n",
    "y0 = 1.0\n",
    "crit = (p - y0).roots # roots of the polynominal -1, i.e., the mz values where p = 1 \n",
    "\n",
    "#shading in the uncertainty band \n",
    "px=np.arange(crit[1],crit[0],0.001)\n",
    "ax.fill_between(px,p(px),alpha=0.5, color='g', label=\"uncertainty\")\n",
    "ax.legend()\n",
    "\n",
    "#setting reasonable axis ranges and titles\n",
    "ax.set_xlim(minMz, maxMz)\n",
    "ax.set_ylim(0.0, 5.0)\n",
    "plt.xlabel(\"mZ [MeV]\")\n",
    "plt.ylabel(\"delta chi-squared\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#print result and uncertainty\n",
    "my1DResult = bestFitMz1D\n",
    "my1DUncertainty = bestFitMz1D-crit[1]\n",
    "print(\"best-fit = \" + str(my1DResult) + \" +/- \" + str(my1DUncertainty))\n",
    "\n",
    "# we now compare our data histogram with the cauchy distribution with the mean and standard deviation\n",
    "# that minimise the chi-squared. This pdf should agree much better than the pdf with the PDG values.\n",
    "\n",
    "plt.figure()\n",
    "p = (cauchy.pdf(centres, my1DResult, sigmaZPDG))\n",
    "\n",
    "pInt = bin_width * sum(p[0:nBins])\n",
    "scale = integral/pInt\n",
    "pScaled = (cauchy.pdf(centres, my1DResult, sigmaZPDG) *scale)\n",
    "\n",
    "plt.plot(centres, pScaled, 'k', linewidth=2)\n",
    "\n",
    "x = np.linspace(87000, 94000, 1000)\n",
    "pScaledNorm = (norm.pdf(x, my1DResult, (sigmaZPDG*0.8) )*(0.6*scale))\n",
    "\n",
    "#plt.plot(x, pScaledNorm, 'r', linewidth=2)\n",
    "\n",
    "plt.hist(lvArray.mass, bins=nBins, range=[minMass, maxMass], alpha=0.6, color='g')\n",
    "plt.xlabel(\"mumu mass [MeV]\")\n",
    "plt.ylabel(\"# events\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a double gaussian with curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def gaussian(x, mean, amplitude, standard_deviation):\n",
    "    return amplitude * np.exp( - ((x - mean) / standard_deviation) ** 2)\n",
    "\n",
    "def doubleGaussian(x, mean1, amplitude1, standard_deviation1, mean2, amplitude2, standard_deviation2):\n",
    "    gaus1 = amplitude1 * np.exp( - ((x - mean1) / standard_deviation1) ** 2)\n",
    "    gaus2 = amplitude2 * np.exp( - ((x - mean2) / standard_deviation2) ** 2)\n",
    "    return gaus1 + gaus2\n",
    "\n",
    "#bestFitParams, covarianceMatrix = curve_fit(gaussian, centres, countsData, p0=[91000., 2000., 10000.])\n",
    "bestFitParams, covarianceMatrix = curve_fit(doubleGaussian, centres, countsData, p0=[91000., 2000., 10000., 91000, 6000, 2000])\n",
    "\n",
    "x_interval_for_fit = np.linspace(edges[0], edges[-1], 10000)\n",
    "#plt.plot(x_interval_for_fit, gaussian(x_interval_for_fit, *bestFitParams), label='gaussian fit')\n",
    "plt.plot(x_interval_for_fit, doubleGaussian(x_interval_for_fit, *bestFitParams),'r-', linewidth=3, label='double gaussian fit')\n",
    "plt.errorbar(centres, countsData, yerr=np.sqrt(countsData), label=\"Data\", fmt='o', mfc='k', mec='k', mew=0.1, ecolor='k')\n",
    "plt.legend()\n",
    "plt.xlabel(\"$ m_{\\mu\\mu}$\")\n",
    "plt.ylabel(\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have a 'measurement' of mZ, let's make a plot comparing our measured value with that from\n",
    "the PDG to assess how good our measurement is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mZPDGAr = np.array([mZPDG]) \n",
    "uncZPDG = 2.1 #MeV\n",
    "uncZPDGAr = np.array([uncZPDG]) # https://pdg.lbl.gov/2018/listings/rpp2018-list-z-boson.pdf\n",
    "yPDGAr = np.array([1.0]) \n",
    "\n",
    "mZ1D = np.array([my1DResult]) \n",
    "sigZ1D = np.array([my1DUncertainty]) \n",
    "y1D = np.array([2.0]) \n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(minMz, maxMz+700)#need to extend x axis range to include PDG value\n",
    "ax.set_ylim(0.0, 4.0)\n",
    "\n",
    "plt.errorbar(mZPDGAr, yPDGAr, xerr=uncZPDGAr, label=\"Particle Data Group\", elinewidth=5,  fmt=\"*\", mfc=\"blue\", ms=8)\n",
    "plt.errorbar(mZ1D, y1D, xerr=sigZ1D, fmt=\"*\", label=\"ATLAS Open Data\", elinewidth=5, ms=8, mfc='red')\n",
    "\n",
    "plt.xlabel(\"mZ [MeV]\")\n",
    "plt.yticks(y1D, \" \")\n",
    "\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats completes steps 1-5. You have seen how the chi-squared minimisation technique allows us to extract measurements\n",
    "of a physical parameter such as $m_{Z}$ by comparing the predictions of statistical model to data.\n",
    "\n",
    "# Now it's your turn!\n",
    "\n",
    "Refer back to the lab manual to see how you might improve this measurement and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your explorations go here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional extra activity\n",
    "\n",
    "The previous activities complete the lab and you should receive a great grade if you complete them all, add your own investigations on modifications of the analysis and their interpretation as well as writing up a report that is clear and detailed.\n",
    "\n",
    "However, I wanted to show you some more advanced and systematics explorations of the data and analysis that will demonstrate the ways particle physicists interrogate their own results to check for potential systematic biases and sources of uncertainty that may not be immediatley apparent. You will have likely obtained a result for the Z mass that does not agree with the PDG value. This is not neccessarily a problem: we can never fully rule out the possibility that the PDG value is wrong and your result is correct. Even if there was agreement between the two values, we would still want to check for systematic uncertainties in our analysis. One way this could show up is a string dependence of our results on the 'parameters' of our analysis, e.g, the cuts we apply to the data or the particular fit model. Small changes to these parameters should not grealty affect our results unless there are some systematic biases at play. Hence we should estimate how much our result depends on these parameters and calculate an extra, \"systematic\" uncetainty for our result.\n",
    "\n",
    "A rigorous way to do this is to systematically scan over multiple options for our analysis parameters, extra results for each option and finally plot the results as a function of the parameters.\n",
    "\n",
    "To do this, we will have to advance our coding a little, such that it is easy to automatically run our full analysis with any set of analysis parameters. Practically, we write the entire analysis that \"measures\" the Z mass into a single python function that takes the analysis parmeters as inputs an returns the measure Z mass and standard devaiation. We then plot the results as a function of the parameter choices. Note that for simplicity and speed, I replaced our more precise chi-squared fit with a simpler, least-squares fit from curve fit. This is not sufficiently precise to calculate our final result of statistical uncertainty, but is good enough to check for the systematic biases mentioned.\n",
    "\n",
    "# Move on to Part 2 of this lab before attempting this optional extra activity!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the z mass analysis as one function\n",
    "def measure_z_mass(data_df, cut_values, pdf_name):\n",
    "    # Apply event selections based on the provided cut values\n",
    "    cuts = data_df.copy()\n",
    "    cuts = cuts[cuts['muon_charge'].apply(lambda x: x[0] + x[1] == 0)]\n",
    "    cuts = cuts[(cuts[\"muon_pt\"]).apply(lambda x: x[0] > cut_values['pt_cut']) & (cuts[\"muon_pt\"]).apply(lambda x: x[1] > cut_values['pt_cut'])]\n",
    "    cuts = cuts[(cuts[\"muon_eta\"]).apply(lambda x: np.abs(x[0]) < cut_values['eta_cut']) & (cuts[\"muon_eta\"]).apply(lambda x: np.abs(x[0]) < cut_values['eta_cut'])]\n",
    "    \n",
    "    # Apply isolation criteria if provided\n",
    "    if 'iso_et_cut' in cut_values:\n",
    "        cuts['iso_et20_mu0'] = (pd.Series(cuts['muon_etcone20']).apply(lambda x: x[0]) / pd.Series(cuts['muon_pt']).apply(lambda x: x[0]))\n",
    "        cuts['iso_et20_mu1'] = (pd.Series(cuts['muon_etcone20']).apply(lambda x: x[1]) / pd.Series(cuts['muon_pt']).apply(lambda x: x[1]))\n",
    "        cuts = cuts[(cuts['iso_et20_mu0'] < cut_values['iso_et_cut']) & (cuts['iso_et20_mu1'] < cut_values['iso_et_cut'])]\n",
    "    \n",
    "    # Reconstruct the Z boson\n",
    "    pt0 = cuts['muon_pt'].apply(lambda x: x[0]).to_numpy()\n",
    "    eta0 = cuts['muon_eta'].apply(lambda x: x[0]).to_numpy()\n",
    "    phi0 = cuts['muon_phi'].apply(lambda x: x[0]).to_numpy()\n",
    "    E0 = cuts['muon_E'].apply(lambda x: x[0]).to_numpy()\n",
    "    \n",
    "    pt1 = cuts['muon_pt'].apply(lambda x: x[1]).to_numpy()\n",
    "    eta1 = cuts['muon_eta'].apply(lambda x: x[1]).to_numpy()\n",
    "    phi1 = cuts['muon_phi'].apply(lambda x: x[1]).to_numpy()\n",
    "    E1 = cuts['muon_E'].apply(lambda x: x[1]).to_numpy()\n",
    "    \n",
    "    lvArray0 = vector.array({\"pt\": pt0, \"phi\": phi0, \"eta\": eta0, \"E\": E0})\n",
    "    lvArray1 = vector.array({\"pt\": pt1, \"phi\": phi1, \"eta\": eta1, \"E\": E1})\n",
    "    lvArray = lvArray0 + lvArray1\n",
    "    \n",
    "    # Create a histogram of the dimuon mass distribution\n",
    "    min_mass, max_mass = 70000, 110000\n",
    "    n_bins = 100\n",
    "    counts, edges = np.histogram(lvArray.mass, bins=n_bins, range=(min_mass, max_mass))\n",
    "    bin_centers = (edges[1:] + edges[:-1]) / 2\n",
    "    bin_width = edges[1] - edges[0]\n",
    "    \n",
    "    # Calculate the sum of bin counts\n",
    "    sum_counts = np.sum(counts)\n",
    "    \n",
    "    # Define the fit function based on the provided PDF name\n",
    "    if pdf_name == 'cauchy':\n",
    "        def fit_func(x, loc, scale, intgrl):\n",
    "            pdf = cauchy.pdf(x, loc, scale)\n",
    "            return intgrl * pdf / np.sum(pdf) * sum_counts\n",
    "    elif pdf_name == 'gaussian':\n",
    "        def fit_func(x, mean, std, intgrl):\n",
    "            pdf = norm.pdf(x, mean, std)\n",
    "            return intgrl * pdf / np.sum(pdf) * sum_counts\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported PDF: {pdf_name}\")\n",
    "    \n",
    "    # Perform the fit with initial parameter guesses\n",
    "    p0 = [91000, 2000, 1.0]  # Initial guesses for mean, std, and norm\n",
    "    params, p_cov = curve_fit(fit_func, bin_centers, counts, p0=p0)\n",
    "    \n",
    "    # Extract the fitted Z mass, uncertainty, and scaling factor\n",
    "    if pdf_name == 'cauchy':\n",
    "        z_mass = params[0]\n",
    "        z_mass_sigma = params[1]\n",
    "        scaling_factor = params[2]\n",
    "    elif pdf_name == 'gaussian':\n",
    "        z_mass = params[0]\n",
    "        z_mass_sigma = params[1]\n",
    "        scaling_factor = params[2]\n",
    "    \n",
    "    # Calculate the chi-squared value and degrees of freedom\n",
    "    fit_counts = fit_func(bin_centers, *params)\n",
    "    chi_squared = np.sum((counts - fit_counts)**2 / fit_counts)\n",
    "    dof = n_bins - len(params)\n",
    "    \n",
    "    \n",
    "    # Return all the necessary details\n",
    "    return {\n",
    "        'z_mass': z_mass,\n",
    "        'z_mass_sigma': z_mass_sigma,\n",
    "        'scaling_factor': scaling_factor,\n",
    "        'bin_centers': bin_centers,\n",
    "        'bin_width': bin_width,\n",
    "        'counts': counts,\n",
    "        'fit_counts': fit_counts,\n",
    "        'chi_squared': chi_squared,\n",
    "        'dof': dof\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of cut values and PDFs to test\n",
    "\n",
    "#these first set of options will take quite some time to run\n",
    "#pt_cuts = [20000, 25000, 30000]\n",
    "#eta_cuts = [2.0, 2.4, 2.8]\n",
    "#iso_et_cuts = [0.1, 0.2, 0.3]\n",
    "#pdfs = ['cauchy', 'gaussian']\n",
    "\n",
    "# hence first test with this shorter set (takes around 5 minutes to run)\n",
    "# Define a range of cut values and PDFs to test\n",
    "pt_cuts = [20000, 40000]\n",
    "eta_cuts = [1.5, 2.4]\n",
    "iso_et_cuts = [0.1, 0.5]\n",
    "pdfs = ['cauchy','gaussian']\n",
    "\n",
    "results = []\n",
    "\n",
    "for pt_cut in pt_cuts:\n",
    "    for eta_cut in eta_cuts:\n",
    "        for iso_et_cut in iso_et_cuts:\n",
    "            for pdf in pdfs:\n",
    "                cut_values = {'pt_cut': pt_cut, 'eta_cut': eta_cut, 'iso_et_cut': iso_et_cut}\n",
    "                fit_results = measure_z_mass(df, cut_values, pdf)\n",
    "                \n",
    "                result = {\n",
    "                    'pt_cut': pt_cut,\n",
    "                    'eta_cut': eta_cut,\n",
    "                    'iso_et_cut': iso_et_cut,\n",
    "                    'pdf': pdf,\n",
    "                    'z_mass': fit_results['z_mass'],\n",
    "                    'z_mass_sigma': fit_results['z_mass_sigma'],\n",
    "                    'bin_centers': fit_results['bin_centers'],\n",
    "                    'bin_width': fit_results['bin_width'],\n",
    "                    'counts': fit_results['counts'],\n",
    "                    'fit_counts': fit_results['fit_counts'],\n",
    "                    'scaling_factor': fit_results['scaling_factor'],\n",
    "                    'chi_squared': fit_results['chi_squared'],\n",
    "                    'dof': fit_results['dof']\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the results to check for systematic biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "min_mass, max_mass = 70000, 110000\n",
    "n_bins = 100\n",
    "\n",
    "# Create a grid of subplots for each combination of parameters\n",
    "fig, axes = plt.subplots(len(eta_cuts), len(iso_et_cuts), figsize=(12, 10), sharex=True, sharey=True)\n",
    "\n",
    "for i, eta_cut in enumerate(eta_cuts):  # this code will only work if you scan over multiple eta values\n",
    "    for j, iso_et_cut in enumerate(iso_et_cuts):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        for pdf in pdfs:\n",
    "            data = results_df[(results_df['eta_cut'] == eta_cut) & (results_df['iso_et_cut'] == iso_et_cut) & (results_df['pdf'] == pdf)]\n",
    "            \n",
    "            ax.errorbar(data['pt_cut'], data['z_mass'], yerr=data['z_mass_sigma'], fmt='-o', label=pdf)\n",
    "        \n",
    "        # Add a horizontal semi-transparent dashed line for the \"true\" Z mass value\n",
    "        ax.axhline(y=mZPDG, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add a semi-transparent band indicating the uncertainty on the PDG values\n",
    "        ax.axhspan(mZPDG - uncZPDG, mZPDG + uncZPDG, color='gray', alpha=0.2, label=r'$m_{Z}$ PDG')\n",
    "        \n",
    "        ax.set_title(f'eta_cut = {eta_cut}, iso_et_cut = {iso_et_cut}')\n",
    "        ax.set_xlabel('pt_cut')\n",
    "        ax.set_ylabel('Fitted Z Mass')\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a new figure with the fit results for each combination of parameters\n",
    "fig2, axes2 = plt.subplots(len(eta_cuts), len(iso_et_cuts), figsize=(12, 10), sharex=True, sharey=True)\n",
    "\n",
    "for i, eta_cut in enumerate(eta_cuts):\n",
    "    for j, iso_et_cut in enumerate(iso_et_cuts):\n",
    "        ax2 = axes2[i, j]\n",
    "        \n",
    "        for pdf in pdfs:\n",
    "            data = results_df[(results_df['eta_cut'] == eta_cut) & (results_df['iso_et_cut'] == iso_et_cut) & (results_df['pdf'] == pdf)]\n",
    "            \n",
    "            for _, row in data.iterrows():\n",
    "                pt_cut = row['pt_cut']\n",
    "                pdf_name = row['pdf']\n",
    "                z_mass = row['z_mass']\n",
    "                z_mass_sigma = row['z_mass_sigma']\n",
    "                bin_centers = row['bin_centers']\n",
    "                bin_width = row['bin_width']\n",
    "                counts = row['counts']\n",
    "                fit_counts = row['fit_counts']\n",
    "                \n",
    "                # Plot the dimuon mass distribution histogram and fitted function\n",
    "                ax2.hist(bin_centers, bins=len(bin_centers), weights=counts, histtype='step', alpha=0.5, label=f'Data (pt_cut = {pt_cut}, pdf = {pdf_name})')\n",
    "                ax2.plot(bin_centers, fit_counts, label=f'Fit (pt_cut = {pt_cut}, pdf = {pdf_name})')\n",
    "        \n",
    "        ax2.set_title(f'eta_cut = {eta_cut}, iso_et_cut = {iso_et_cut}')\n",
    "        ax2.set_xlabel('Dimuon Mass [MeV]')\n",
    "        ax2.set_ylabel('Events')\n",
    "        ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
